{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2: Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification is the task of sorting text documents into predefined classes. The concrete problem you will be working on in this lab is the classification of texts with respect to their political affiliation. The specific texts you are going to classify are speeches held in the [Riksdag](https://www.riksdagen.se/en/), the Swedish national legislature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data for this lab comes from [The Riksdag’s Open Data](https://data.riksdagen.se/in-english/). We have tokenized the speeches and put them into two compressed [JSON](https://en.wikipedia.org/wiki/JSON) files:\n",
    "\n",
    "* `speeches-201718.json.bz2` (speeches from the 2017/2018 parliamentary session)\n",
    "* `speeches-201819.json.bz2` (ditto, from the 2018/2019 session)\n",
    "\n",
    "We start by loading these files into two separate data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bz2\n",
    "\n",
    "with bz2.open(\"speeches-201718.json.bz2\") as source:\n",
    "    speeches_201718 = pd.read_json(source)\n",
    "\n",
    "with bz2.open(\"speeches-201819.json.bz2\") as source:\n",
    "    speeches_201819 = pd.read_json(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you inspect the two data frames, you can see that there are three labelled columns: `id` (the official speech ID), `words` (the space-separated words of the speech), and `party` (the party of the speaker, represented by its customary abbreviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_201718.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the lab, we will be using the speeches from 2017/2018 as our training data, and the speeches from 2018/2019 as our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data = speeches_201718, speeches_201819"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later reference, we store the sorted list of party abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parties = sorted(training_data[\"party\"].unique())\n",
    "print(parties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to get to know the data better by plotting a simple visualization.\n",
    "\n",
    "If you are not familiar with the Swedish political system and the parties represented in the Riksdag in particular, then we suggest that you have a look at the Wikipedia article about the [2018 Swedish general election](https://en.wikipedia.org/wiki/2018_Swedish_general_election).\n",
    "\n",
    "For the lab, we ask you to compare the two data frames with respect to the distribution of the speeches over the different parties. Write code to generate two bar plots that visualize this information, one for the 2017/2018 speeches and one for the 2018/2019 speeches. Inspect the two plots, and compare them\n",
    "\n",
    "* to each other\n",
    "* to the results of the 2014 and the 2018 general elections\n",
    "\n",
    "Summarize your observations in a short text in the cell below.\n",
    "\n",
    "**Tip:** If you need help with creating bar plots, [Bar Plot using Pandas](https://dfrieds.com/data-visualizations/bar-plot-python-pandas) provides a useful tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "training_data['party'].value_counts().plot(kind='bar');\n",
    "plt.title(\"Distribution of the speeches over the different parties for the 2017/2018\", y=1.02);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['party'].value_counts().plot(kind='bar');\n",
    "plt.title(\"Distribution of the speeches over the different parties for the 2018/2019\", y=1.02);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Enter your summary here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to train and evaluate a classifier. More specifically, we ask you to train a [Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) classifier. You will have to\n",
    "\n",
    "1. vectorize the speeches in the training data\n",
    "2. instantiate and fit the Naive Bayes model\n",
    "3. evaluate the model on the test data\n",
    "\n",
    "The scikit-learn library provides a convenience class [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) that allows you to solve the first two tasks with very compact code. For the evaluation you can use the function [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html), which will report per-class precision, recall and F1, as well as overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code here to train and evaluate a Multinomial Naive Bayes classifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_data['words']\n",
    "Y_train = training_data['party']\n",
    "X_test = test_data['words']\n",
    "Y_test = test_data['party']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           C       0.00      0.00      0.00         0\n",
      "          KD       0.00      0.00      0.00         0\n",
      "           L       0.00      0.00      0.00         0\n",
      "           M       0.00      0.50      0.00         2\n",
      "          MP       0.00      0.00      0.00         0\n",
      "           S       1.00      0.30      0.46      9286\n",
      "          SD       0.00      0.00      0.00         0\n",
      "           V       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      9288\n",
      "   macro avg       0.13      0.10      0.06      9288\n",
      "weighted avg       1.00      0.30      0.46      9288\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wujwgary/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('model', MultinomialNB()),\n",
    "])\n",
    "Y_pred=pipeline.fit(X_train,Y_train).predict(X_test)\n",
    "print(classification_report(Y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           C       0.04      0.63      0.07        41\n",
      "          KD       0.02      0.70      0.03        20\n",
      "           L       0.02      0.92      0.04        12\n",
      "           M       0.68      0.36      0.47      3110\n",
      "          MP       0.25      0.36      0.29       552\n",
      "           S       0.84      0.46      0.59      5078\n",
      "          SD       0.12      0.57      0.20       232\n",
      "           V       0.15      0.59      0.24       243\n",
      "\n",
      "   micro avg       0.43      0.43      0.43      9288\n",
      "   macro avg       0.26      0.57      0.24      9288\n",
      "weighted avg       0.71      0.43      0.51      9288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, Y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "print(classification_report(y_pred_class,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics such as accuracy should not be understood as absolute measures of performance, but should be used only to compare different classifiers. When other classifiers are not available, a simple baseline for text classification is **Most Frequent Class (MFC)**. One way to think of this baseline is as a classifier that, for every document, predicts that class which appears most often in the training data.\n",
    "\n",
    "Determine the most frequent class in the 2017/2018 data. What is the accuracy of the MFC baseline on the test data? Given this baseline accuracy, how do you assess the results of the Naive Bayes classifier from Problem&nbsp;2? Answer with a short text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code here to print the baseline accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Enter your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Creating a balanced data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you saw in Problem&nbsp;1, the distribution of the speeches over the eight different parties (classes) is imbalanced. One technique used to alleviate this is **undersampling**, in which one randomly removes samples from over-represented classes until all classes are represented with the same number of samples.\n",
    "\n",
    "Implement undersampling to create a balanced subset of the training data. Rerun the evaluation from Problem&nbsp;2 on the balanced data and compare the results. Discuss your findings in a short text. Would you argue that undersampling make sense for the task of predicting the party of a speaker?\n",
    "\n",
    "**Hint:** Your balanced subset should consist of 5,752 speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code here to implement undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = Y_train.value_counts().min()\n",
    "df = pd.DataFrame()\n",
    "for p in parties:\n",
    "    newdf = training_data[training_data[\"party\"]==p].sample(n=num)\n",
    "    df = pd.concat([df,newdf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           C       0.43      0.27      0.33      1082\n",
      "          KD       0.38      0.30      0.34      1044\n",
      "           L       0.44      0.27      0.33       914\n",
      "           M       0.46      0.41      0.43      1861\n",
      "          MP       0.35      0.36      0.36       795\n",
      "           S       0.34      0.79      0.47      1182\n",
      "          SD       0.43      0.43      0.43      1061\n",
      "           V       0.55      0.39      0.46      1349\n",
      "\n",
      "   micro avg       0.41      0.41      0.41      9288\n",
      "   macro avg       0.42      0.40      0.39      9288\n",
      "weighted avg       0.43      0.41      0.40      9288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_new = df['words']\n",
    "Y_train_new = df['party']\n",
    "\n",
    "Y_pred=pipeline.fit(X_train_new,Y_train_new).predict(X_test)\n",
    "print(classification_report(Y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **confusion matrix** is a specific table layout that is useful when analysing the performance of a classifier. In this matrix, both the rows and the columns correspond to classes, and each cell $(i, j)$ states how many times a sample with gold-standard class $i$ was predicted as belonging to class $j$.\n",
    "\n",
    "In scitkit-learn, the confusion matrix of a classifier is computed by the function [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html).\n",
    "\n",
    "Your task is to use the confusion matrix to find, for each given party $p$ in the Riksdag, that other party $p'$ which the classifier that you trained in Problem&nbsp;4 most often confuses $p$ with when it predicts the party of a speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[290,  65,  53, 105,  32,  19,  52,  55],\n",
       "       [ 81, 314,  54, 171,  31,  23,  71,  76],\n",
       "       [ 60,  47, 246,  64,  22,  20,  38,  63],\n",
       "       [201, 179, 145, 754,  81,  49, 127, 108],\n",
       "       [ 89,  59,  61,  80, 285,  91,  53,  91],\n",
       "       [209, 222, 188, 418, 284, 934, 190, 328],\n",
       "       [ 97,  87, 113, 154,  30,  19, 456, 104],\n",
       "       [ 55,  71,  54, 115,  30,  27,  74, 524]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Write code here to solve Problem 5\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, you have been using the vectorizer and the Naive Bayes classifier with their default hyperparameters. When working with real-world applications, you would want to find settings for the hyperparameters that maximize the performance for the task at hand.\n",
    "\n",
    "Manually tweaking the hyperparameters of the various components of a vectorizer–classifier pipeline can be cumbersome. However, scikit-learn makes it possible to run an exhaustive search for the best hyperparameters over a grid of possible values. This method is known as **grid search**.\n",
    "\n",
    "The hyperparameters of a pipeline should never be tuned on the final test set. (Why would that be a bad idea?) Instead, one should either use a separate validation set, or run cross-validation over different folds. Here we will use cross-validation.\n",
    "\n",
    "Implement a grid search with 5-fold cross-validation to find the optimal parameters in a grid defined by the following choices for the hyperparameters:\n",
    "\n",
    "* In the vectorizer, try a set-of-words model instead of the default bag-of-words model (two possible parameter values).\n",
    "* Also in the vectorizer, try extracting $n$-grams up to $n = 2$ (two possible parameter values).\n",
    "* In the Naive Bayes classifier, try using additive smoothing with $\\alpha \\in \\{1, 0{.}1\\}$ (two possible parameter values).\n",
    "\n",
    "Use the class [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) from the scikit-learn library. Print the results of your best model, along with the parameter values that yielded these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code here to implement the grid search\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = {'vect__analyzer':('char', 'char_wb'),'vect__ngram_range':[(1, 2)],'model__alpha':(1, 0.1)}\n",
    "clf = GridSearchCV(pipeline,parameter,cv=5,n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ear_tf=False, use_idf=True)), ('model', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'vect__analyzer': ('char', 'char_wb'), 'vect__ngram_range': [(1, 2)], 'model__alpha': (1, 0.1)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_new,Y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__alpha': 0.1, 'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3605702364394993"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wujwgary/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/wujwgary/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/wujwgary/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/wujwgary/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/wujwgary/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/wujwgary/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/wujwgary/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([10.25020595, 17.52514715, 11.01271839, 16.70956335]),\n",
       " 'std_fit_time': array([0.04284291, 0.39953832, 0.13564455, 2.6235451 ]),\n",
       " 'mean_score_time': array([2.96009417, 5.10678563, 3.22633882, 3.34582329]),\n",
       " 'std_score_time': array([0.04862562, 0.13849036, 0.07280962, 0.8428069 ]),\n",
       " 'param_model__alpha': masked_array(data=[1, 1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__analyzer': masked_array(data=['char', 'char_wb', 'char', 'char_wb'],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__ngram_range': masked_array(data=[(1, 2), (1, 2), (1, 2), (1, 2)],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'model__alpha': 1,\n",
       "   'vect__analyzer': 'char',\n",
       "   'vect__ngram_range': (1, 2)},\n",
       "  {'model__alpha': 1,\n",
       "   'vect__analyzer': 'char_wb',\n",
       "   'vect__ngram_range': (1, 2)},\n",
       "  {'model__alpha': 0.1, 'vect__analyzer': 'char', 'vect__ngram_range': (1, 2)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'vect__analyzer': 'char_wb',\n",
       "   'vect__ngram_range': (1, 2)}],\n",
       " 'split0_test_score': array([0.33940972, 0.33072917, 0.34288194, 0.35069444]),\n",
       " 'split1_test_score': array([0.33680556, 0.32638889, 0.35503472, 0.35069444]),\n",
       " 'split2_test_score': array([0.37413194, 0.35243056, 0.38628472, 0.38802083]),\n",
       " 'split3_test_score': array([0.35416667, 0.34288194, 0.3515625 , 0.35590278]),\n",
       " 'split4_test_score': array([0.3479021 , 0.34440559, 0.36363636, 0.35751748]),\n",
       " 'mean_test_score': array([0.35048679, 0.33936022, 0.35987483, 0.36057024]),\n",
       " 'std_test_score': array([0.01334454, 0.00950677, 0.01479414, 0.01400714]),\n",
       " 'rank_test_score': array([3, 4, 2, 1], dtype=int32),\n",
       " 'split0_train_score': array([0.44565217, 0.42869565, 0.49130435, 0.49086957]),\n",
       " 'split1_train_score': array([0.44673913, 0.42782609, 0.47826087, 0.47347826]),\n",
       " 'split2_train_score': array([0.44086957, 0.42130435, 0.47847826, 0.47391304]),\n",
       " 'split3_train_score': array([0.44304348, 0.42282609, 0.46934783, 0.47021739]),\n",
       " 'split4_train_score': array([0.43793403, 0.41189236, 0.47829861, 0.47265625]),\n",
       " 'mean_train_score': array([0.44284768, 0.42250891, 0.47913798, 0.4762269 ]),\n",
       " 'std_train_score': array([0.00319699, 0.0060135 , 0.00701109, 0.0074321 ])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: Try to improve your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn makes it easy to test different vectorizer–classifier pipelines – among other things, it includes different types of logistic regression classifiers, support vector machines, and decision trees. Browse the library to see which methods are supported.\n",
    "\n",
    "Build a pipeline that you find interesting, and use grid search to find optimal settings for the hyperparameters. Print the results of your best model. Did you manage to get better results than the ones that you obtained in Problem&nbsp;5? Answer with a short text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'vect__analyzer': ('char', 'char_wb'), 'vect__ngram_range': [(1, 2)], 'model__max_depth': [7, 8]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Write code here to search for a better model\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "pipeline_new = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('model', DecisionTreeClassifier()),\n",
    "])\n",
    "#pipeline.fit(X_train,Y_train).predict(X_test)\n",
    "parameter_new = {'vect__analyzer':('char', 'char_wb'),'vect__ngram_range':[(1, 2)],'model__max_depth':[7,8]}\n",
    "clf_new = GridSearchCV(pipeline_new,parameter_new,cv=5,n_jobs=-1)\n",
    "clf_new.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__max_depth': 7, 'vect__analyzer': 'char', 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_new.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.371384590456129"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_new.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wujwgary/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/wujwgary/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/wujwgary/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/wujwgary/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/wujwgary/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/wujwgary/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/wujwgary/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([22.18984809, 36.53980455, 26.37851763, 35.56063061]),\n",
       " 'std_fit_time': array([0.16958182, 1.42355222, 0.15529459, 5.14102036]),\n",
       " 'mean_score_time': array([5.84019651, 9.63381033, 6.20746799, 6.44647684]),\n",
       " 'std_score_time': array([0.23470694, 0.50012513, 0.22876725, 1.49149195]),\n",
       " 'param_model__max_depth': masked_array(data=[7, 7, 8, 8],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__analyzer': masked_array(data=['char', 'char_wb', 'char', 'char_wb'],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__ngram_range': masked_array(data=[(1, 2), (1, 2), (1, 2), (1, 2)],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'model__max_depth': 7,\n",
       "   'vect__analyzer': 'char',\n",
       "   'vect__ngram_range': (1, 2)},\n",
       "  {'model__max_depth': 7,\n",
       "   'vect__analyzer': 'char_wb',\n",
       "   'vect__ngram_range': (1, 2)},\n",
       "  {'model__max_depth': 8,\n",
       "   'vect__analyzer': 'char',\n",
       "   'vect__ngram_range': (1, 2)},\n",
       "  {'model__max_depth': 8,\n",
       "   'vect__analyzer': 'char_wb',\n",
       "   'vect__ngram_range': (1, 2)}],\n",
       " 'split0_test_score': array([0.36989073, 0.37393768, 0.36624848, 0.36908134]),\n",
       " 'split1_test_score': array([0.36452005, 0.35236938, 0.3600648 , 0.36816525]),\n",
       " 'split2_test_score': array([0.38558121, 0.39206156, 0.37707574, 0.38477116]),\n",
       " 'split3_test_score': array([0.37115073, 0.36750405, 0.36588331, 0.36061588]),\n",
       " 'split4_test_score': array([0.36577453, 0.35644769, 0.36334144, 0.36334144]),\n",
       " 'mean_test_score': array([0.37138459, 0.36846796, 0.36652354, 0.36919712]),\n",
       " 'std_test_score': array([0.00751577, 0.01407952, 0.00572171, 0.008385  ]),\n",
       " 'rank_test_score': array([1, 3, 4, 2], dtype=int32),\n",
       " 'split0_train_score': array([0.43486629, 0.43567666, 0.47072528, 0.46474878]),\n",
       " 'split1_train_score': array([0.43751266, 0.43508203, 0.46657889, 0.47285801]),\n",
       " 'split2_train_score': array([0.42809398, 0.42738505, 0.45483087, 0.45736277]),\n",
       " 'split3_train_score': array([0.44192405, 0.44212658, 0.47220253, 0.47139241]),\n",
       " 'split4_train_score': array([0.43009011, 0.43798724, 0.46481725, 0.47494178]),\n",
       " 'mean_train_score': array([0.43449742, 0.43565151, 0.46583097, 0.46826075]),\n",
       " 'std_train_score': array([0.00499622, 0.0048175 , 0.00611785, 0.00642928])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_new.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...f', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'vect__analyzer': ('char', 'char_wb'), 'vect__ngram_range': [(1, 2)], 'model__kernel': ('linear', 'rbf')},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "pipeline_new1 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('model', SVC()),\n",
    "])\n",
    "#pipeline.fit(X_train,Y_train).predict(X_test)\n",
    "parameter_new1 = {'vect__analyzer':('char', 'char_wb'),'vect__ngram_range':[(1, 2)],'model__kernel':('linear','rbf')}\n",
    "clf_new1 = GridSearchCV(pipeline_new1,parameter_new1,cv=5,n_jobs=-1)\n",
    "clf_new1.fit(X_train_new,Y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__kernel': 'linear',\n",
       " 'vect__analyzer': 'char',\n",
       " 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_new1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24165507649513213"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_new1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Enter your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    Please read the section ‘General information’ on the ‘Labs’ page of the course website before submitting this notebook!\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
